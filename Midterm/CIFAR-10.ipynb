{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b0ca19",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import BatchGrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5c021",
   "metadata": {},
   "source": [
    "ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Global variable for the number of groups used in Group Normalization.\n",
    "gn_groups = 4\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"\n",
    "    3x3 convolution with padding.\n",
    "    \n",
    "    Args:\n",
    "        in_planes (int): Number of input channels.\n",
    "        out_planes (int): Number of output channels.\n",
    "        stride (int, optional): Stride for the convolution. Default is 1.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Conv2d: A 3x3 convolution layer with the specified parameters.\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block that forms the building component of ResNet.\n",
    "    \n",
    "    It consists of two 3x3 convolutional layers each followed by Group Normalization \n",
    "    and ReLU activation. If the dimensions need to be matched, a downsampling operation \n",
    "    is applied to the identity branch.\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        \"\"\"\n",
    "        Initialize the BasicBlock module.\n",
    "        \n",
    "        Args:\n",
    "            inplanes (int): Number of channels in the input tensor.\n",
    "            planes (int): Number of channels produced by the block.\n",
    "            stride (int, optional): Stride for the first convolutional layer. Defaults to 1.\n",
    "            downsample (nn.Module, optional): Optional downsampling module to match dimensions.\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # First convolutional layer with a 3x3 kernel.\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        # Group Normalization applied after the first convolution.\n",
    "        self.gn1 = nn.GroupNorm(gn_groups, planes, affine=False)\n",
    "        # ReLU activation (inplace set to False for clarity).\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        # Second convolutional layer.\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        # Group Normalization applied after the second convolution.\n",
    "        self.gn2 = nn.GroupNorm(gn_groups, planes, affine=False)\n",
    "\n",
    "        # Downsampling module if dimension matching is required.\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the BasicBlock.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying residual connections.\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "\n",
    "        # First convolution -> group normalization -> ReLU activation.\n",
    "        out = self.conv1(x)\n",
    "        out = self.gn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Second convolution -> group normalization.\n",
    "        out = self.conv2(out)\n",
    "        out = self.gn2(out)\n",
    "\n",
    "        # If downsampling is set, adjust the identity branch.\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            # Concatenate zeros along the channel dimension to match the output dimensions.\n",
    "            identity = torch.cat((identity, torch.zeros_like(identity)), 1).clone()\n",
    "\n",
    "        # Add the residual connection and apply the final activation.\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a Residual Network (ResNet).\n",
    "    \n",
    "    The network includes an initial convolutional layer followed by multiple residual layers,\n",
    "    adaptive average pooling, and a final fully connected layer for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        \"\"\"\n",
    "        Initialize the ResNet model.\n",
    "        \n",
    "        Args:\n",
    "            block (nn.Module): Type of residual block to use (e.g., BasicBlock).\n",
    "            layers (list): A list defining the number of blocks in each residual layer.\n",
    "            num_classes (int, optional): Number of classes for classification. Defaults to 10.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.num_layers = sum(layers)\n",
    "        self.inplanes = 16\n",
    "        # Initial 3x3 convolution layer for input images (assumes 3-channel input).\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.gn1 = nn.GroupNorm(gn_groups, 16, affine=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        # Build the residual layers.\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        # Global adaptive average pooling to obtain fixed-size feature maps.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Fully connected layer mapping features to class scores.\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Standard weight initialization.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # He initialization for convolutional layers.\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.GroupNorm):\n",
    "                # Initialize GroupNorm's weight to 1 and bias to 0.\n",
    "                try:\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        \"\"\"\n",
    "        Create a residual layer composed of multiple blocks.\n",
    "        \n",
    "        Args:\n",
    "            block (nn.Module): The block type to be used.\n",
    "            planes (int): Number of output channels for this layer.\n",
    "            blocks (int): Number of blocks to be stacked.\n",
    "            stride (int, optional): Stride for the first block. Defaults to 1.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Sequential: Sequential container of residual blocks.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        # If stride is not 1, a downsampling operation is required to adjust dimensions.\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.AvgPool2d(1, stride=stride),\n",
    "                nn.GroupNorm(gn_groups, self.inplanes, affine=False),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # The first block may include downsampling.\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        # Add subsequent blocks without downsampling.\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ResNet model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor containing class scores.\n",
    "        \"\"\"\n",
    "        # Initial convolution -> normalization -> activation.\n",
    "        x = self.conv1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass through the residual layers.\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # Global pooling and flattening.\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Final classification layer.\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet20():\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-20 model.\n",
    "    \n",
    "    Returns:\n",
    "        ResNet: An instance of the ResNet model with 20 layers.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 3, 3])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d966939",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from rdp_accountant import compute_rdp, get_privacy_spent\n",
    "\n",
    "# def process_grad_batch(params, clipping=1):\n",
    "#     n = params[0].grad_batch.shape[0]\n",
    "#     grad_norm_list = torch.zeros(n).cuda()\n",
    "#     for p in params: \n",
    "#         flat_g = p.grad_batch.reshape(n, -1)\n",
    "#         current_norm_list = torch.norm(flat_g, dim=1)\n",
    "#         grad_norm_list += torch.square(current_norm_list)\n",
    "#     grad_norm_list = torch.sqrt(grad_norm_list)\n",
    "#     scaling = clipping/grad_norm_list\n",
    "#     scaling[scaling>1] = 1\n",
    "\n",
    "#     for p in params:\n",
    "#         p_dim = len(p.shape)\n",
    "#         scaling = scaling.view([n] + [1]*p_dim)\n",
    "#         p.grad_batch *= scaling\n",
    "#         p.grad = torch.mean(p.grad_batch, dim=0)\n",
    "#         p.grad_batch.mul_(0.)\n",
    "\n",
    "# def get_data_loader(dataset, batchsize):\n",
    "#     transform_train = transforms.Compose([\n",
    "#     # transforms.RandomCrop(32, padding=4),\n",
    "#     # transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#     ])\n",
    "#     transform_test = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#     ])\n",
    "\n",
    "#     trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "#     trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True, num_workers=2)\n",
    "\n",
    "#     testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test) \n",
    "#     testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=False, num_workers=2)\n",
    "#     return trainloader, testloader, len(trainset), len(testset)\n",
    "\n",
    "# def loop_for_sigma(q, T, eps, delta, cur_sigma, interval, rdp_orders=32, rgp=True):\n",
    "#     while True:\n",
    "#         orders = np.arange(2, rdp_orders, 0.1)\n",
    "#         steps = T\n",
    "#         if(rgp):\n",
    "#             rdp = compute_rdp(q, cur_sigma, steps, orders) * 2 ## when using residual gradients, the sensitivity is sqrt(2)\n",
    "#         else:\n",
    "#             rdp = compute_rdp(q, cur_sigma, steps, orders)\n",
    "#         cur_eps, _, opt_order = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "#         if(cur_eps<eps and cur_sigma>interval):\n",
    "#             cur_sigma -= interval\n",
    "#             previous_eps = cur_eps\n",
    "#         else:\n",
    "#             cur_sigma += interval\n",
    "#             break    \n",
    "#     return cur_sigma, previous_eps\n",
    "\n",
    "# def get_sigma(q, T, eps, delta, init_sigma=10, interval=1., rgp=True):\n",
    "#     cur_sigma = init_sigma\n",
    "    \n",
    "#     cur_sigma, _ = loop_for_sigma(q, T, eps, delta, cur_sigma, interval, rgp=rgp)\n",
    "#     interval /= 10\n",
    "#     cur_sigma, _ = loop_for_sigma(q, T, eps, delta, cur_sigma, interval, rgp=rgp)\n",
    "#     interval /= 10\n",
    "#     cur_sigma, previous_eps = loop_for_sigma(q, T, eps, delta, cur_sigma, interval, rgp=rgp)\n",
    "#     return cur_sigma, previous_eps\n",
    "\n",
    "# def get_lr_scheduler(optimizer, epochs):\n",
    "#     def lr_lamda(epoch):\n",
    "#         if epoch < 10:\n",
    "#             return 0.1 - (0.1 - 0.052) * (epoch / 10)\n",
    "#         else :\n",
    "#             return 0.052\n",
    "#     return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e83ce9",
   "metadata": {},
   "source": [
    "Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_data_loader, get_sigma, checkpoint, adjust_learning_rate, process_grad_batch\n",
    "\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import BatchGrad\n",
    "\n",
    "def train(epoch, net, trainloader, optimizer, loss_func, n_training, batchsize, clip, noise_multiplier, private=True):\n",
    "    \"\"\"\n",
    "    Train the neural network for one epoch.\n",
    "\n",
    "    This function performs a single training epoch using either differentially private \n",
    "    or standard training. When private is True, per-sample gradients are computed using \n",
    "    Backpack, followed by gradient clipping and adding Gaussian noise to enforce \n",
    "    differential privacy.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current training epoch number.\n",
    "        net (nn.Module): The neural network model.\n",
    "        trainloader (DataLoader): DataLoader for the training data.\n",
    "        optimizer (Optimizer): Optimizer for updating the network parameters.\n",
    "        loss_func (function): Loss function used to compute training loss.\n",
    "        n_training (int): Total number of training examples.\n",
    "        batchsize (int): Batch size used for training.\n",
    "        clip (float): Clipping threshold for gradients.\n",
    "        noise_multiplier (float): Multiplier for the Gaussian noise added to gradients.\n",
    "        private (bool, optional): Whether to use differentially private training. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average training loss per step, training accuracy)\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    t0 = time.time()\n",
    "    # Compute the number of steps per epoch based on the training set size and batchsize.\n",
    "    steps = n_training // batchsize\n",
    "\n",
    "    loader = iter(trainloader)\n",
    "\n",
    "    for batch_idx in range(steps):\n",
    "        inputs, targets = next(loader)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        if private:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            # Compute per-sample gradients using Backpack for differential privacy.\n",
    "            with backpack(BatchGrad()):\n",
    "                loss.backward()\n",
    "                # Apply gradient clipping on the per-sample gradients.\n",
    "                process_grad_batch(list(net.parameters()), clip)\n",
    "                # Add Gaussian noise to each parameter's gradient.\n",
    "                for p in net.parameters():\n",
    "                    grad_noise = torch.normal(\n",
    "                        0, noise_multiplier * clip / batchsize,\n",
    "                        size=p.grad.shape, device=p.grad.device\n",
    "                    )\n",
    "                    p.grad.data += grad_noise\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            loss.backward()\n",
    "            # If not using private training, remove per-sample gradients if present.\n",
    "            try:\n",
    "                for p in net.parameters():\n",
    "                    del p.grad_batch\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        optimizer.step()\n",
    "        step_loss = loss.item()\n",
    "        if private:\n",
    "            # Normalize the loss by the number of samples in the batch.\n",
    "            step_loss /= inputs.shape[0]\n",
    "        \n",
    "        train_loss += step_loss\n",
    "        # Compute the number of correct predictions.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).float().cpu().sum()\n",
    "        acc = 100. * float(correct) / float(total)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print('Train loss: %.5f' % (train_loss / (batch_idx + 1)), 'time: %d s' % (t1 - t0), 'train acc:', acc, end=' ')\n",
    "    return (train_loss / batch_idx, acc)\n",
    "\n",
    "\n",
    "def test(epoch, net, testloader, loss_func, private=True):\n",
    "    \"\"\"\n",
    "    Evaluate the neural network on the test dataset.\n",
    "\n",
    "    This function calculates the average loss and accuracy over the test set and saves a \n",
    "    checkpoint if the current accuracy exceeds the best accuracy seen so far.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current epoch number.\n",
    "        net (nn.Module): The neural network model.\n",
    "        testloader (DataLoader): DataLoader for the test dataset.\n",
    "        loss_func (function): Loss function used to compute test loss.\n",
    "        private (bool, optional): Whether the model was trained with differential privacy. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average test loss per step, test accuracy)\n",
    "    \"\"\"\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_correct = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            step_loss = loss.item()\n",
    "            \n",
    "            if private:\n",
    "                # Normalize the loss by the batch size when using private training.\n",
    "                step_loss /= inputs.shape[0]\n",
    "\n",
    "            test_loss += step_loss \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct_idx = predicted.eq(targets.data).cpu()\n",
    "            all_correct += correct_idx.numpy().tolist()\n",
    "            correct += correct_idx.sum()\n",
    "\n",
    "        acc = 100. * float(correct) / float(total)\n",
    "        print('test loss: %.5f' % (test_loss / (batch_idx + 1)), 'test acc:', acc)\n",
    "        \n",
    "        # Save a checkpoint if the current accuracy is higher than the best accuracy.\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            checkpoint(net, acc, epoch, \"resnet20_cifar10\")\n",
    "\n",
    "    return (test_loss / batch_idx, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples:  50000 # of testing examples:  10000\n",
      "\n",
      "==> Computing noise scale for privacy budget (8.0, 0.000010)-DP\n",
      "noise scale:  1.2099999999999993 privacy guarantee:  7.956636368007988\n",
      "\n",
      "==> Creating ResNet20 model instance\n",
      "total number of parameters:  0.268346 M\n",
      "\n",
      "==> Start training\n",
      "\n",
      "Epoch: 0\n",
      "Train loss:2.18650 time: 9 s train acc: 17.954 test loss:1.97736 test acc: 26.77\n",
      "\n",
      "Epoch: 1\n",
      "Train loss:1.91853 time: 9 s train acc: 29.02 test loss:1.89703 test acc: 31.19\n",
      "\n",
      "Epoch: 2\n",
      "Train loss:1.79896 time: 9 s train acc: 34.842 test loss:1.77825 test acc: 37.55\n",
      "\n",
      "Epoch: 3\n",
      "Train loss:1.73391 time: 9 s train acc: 38.904 test loss:1.67740 test acc: 41.95\n",
      "\n",
      "Epoch: 4\n",
      "Train loss:1.66901 time: 9 s train acc: 42.038 test loss:1.77989 test acc: 40.36\n",
      "\n",
      "Epoch: 5\n",
      "Train loss:1.63380 time: 9 s train acc: 43.756 test loss:1.61996 test acc: 45.39\n",
      "\n",
      "Epoch: 6\n",
      "Train loss:1.59258 time: 9 s train acc: 46.258 test loss:1.55403 test acc: 47.55\n",
      "\n",
      "Epoch: 7\n",
      "Train loss:1.57535 time: 9 s train acc: 47.546 test loss:1.59793 test acc: 46.57\n",
      "\n",
      "Epoch: 8\n",
      "Train loss:1.56304 time: 9 s train acc: 48.43 test loss:1.54742 test acc: 49.76\n",
      "\n",
      "Epoch: 9\n",
      "Train loss:1.55846 time: 9 s train acc: 49.024 test loss:1.60575 test acc: 48.06\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         49.02%\n",
      "Testing Accuracy          48.06%\n",
      "Train-Test Gap            0.96%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 10\n",
      "Train loss:1.50700 time: 9 s train acc: 50.88 test loss:1.64995 test acc: 47.78\n",
      "\n",
      "Epoch: 11\n",
      "Train loss:1.50654 time: 9 s train acc: 51.216 test loss:1.50576 test acc: 51.4\n",
      "\n",
      "Epoch: 12\n",
      "Train loss:1.50084 time: 9 s train acc: 52.422 test loss:1.51444 test acc: 52.3\n",
      "\n",
      "Epoch: 13\n",
      "Train loss:1.47326 time: 9 s train acc: 53.176 test loss:1.54150 test acc: 51.23\n",
      "\n",
      "Epoch: 14\n",
      "Train loss:1.51838 time: 9 s train acc: 52.624 test loss:1.60772 test acc: 51.65\n",
      "\n",
      "Epoch: 15\n",
      "Train loss:1.47722 time: 9 s train acc: 53.616 test loss:1.49885 test acc: 52.06\n",
      "\n",
      "Epoch: 16\n",
      "Train loss:1.46737 time: 9 s train acc: 53.902 test loss:1.54047 test acc: 52.2\n",
      "\n",
      "Epoch: 17\n",
      "Train loss:1.47938 time: 9 s train acc: 53.978 test loss:1.52566 test acc: 52.91\n",
      "\n",
      "Epoch: 18\n",
      "Train loss:1.44842 time: 9 s train acc: 55.088 test loss:1.43000 test acc: 55.44\n",
      "\n",
      "Epoch: 19\n",
      "Train loss:1.42971 time: 9 s train acc: 55.47 test loss:1.42925 test acc: 55.51\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         55.47%\n",
      "Testing Accuracy          55.51%\n",
      "Train-Test Gap            -0.04%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 20\n",
      "Train loss:1.41380 time: 9 s train acc: 56.162 test loss:1.46279 test acc: 54.32\n",
      "\n",
      "Epoch: 21\n",
      "Train loss:1.40942 time: 9 s train acc: 55.984 test loss:1.40407 test acc: 55.82\n",
      "\n",
      "Epoch: 22\n",
      "Train loss:1.37908 time: 9 s train acc: 56.906 test loss:1.43632 test acc: 55.67\n",
      "\n",
      "Epoch: 23\n",
      "Train loss:1.39415 time: 9 s train acc: 56.84 test loss:1.41936 test acc: 56.26\n",
      "\n",
      "Epoch: 24\n",
      "Train loss:1.39547 time: 9 s train acc: 56.794 test loss:1.45002 test acc: 55.56\n",
      "\n",
      "Epoch: 25\n",
      "Train loss:1.42133 time: 9 s train acc: 56.476 test loss:1.42457 test acc: 56.85\n",
      "\n",
      "Epoch: 26\n",
      "Train loss:1.35705 time: 9 s train acc: 57.928 test loss:1.43818 test acc: 57.2\n",
      "\n",
      "Epoch: 27\n",
      "Train loss:1.37633 time: 9 s train acc: 57.826 test loss:1.43929 test acc: 56.96\n",
      "\n",
      "Epoch: 28\n",
      "Train loss:1.34760 time: 9 s train acc: 58.35 test loss:1.36620 test acc: 57.68\n",
      "\n",
      "Epoch: 29\n",
      "Train loss:1.34396 time: 9 s train acc: 58.458 test loss:1.36956 test acc: 58.35\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         58.46%\n",
      "Testing Accuracy          58.35%\n",
      "Train-Test Gap            0.11%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 30\n",
      "Train loss:1.34346 time: 9 s train acc: 58.374 test loss:1.34583 test acc: 58.32\n",
      "\n",
      "Epoch: 31\n",
      "Train loss:1.33471 time: 9 s train acc: 59.028 test loss:1.34776 test acc: 58.44\n",
      "\n",
      "Epoch: 32\n",
      "Train loss:1.31735 time: 9 s train acc: 59.11 test loss:1.38278 test acc: 57.71\n",
      "\n",
      "Epoch: 33\n",
      "Train loss:1.32138 time: 9 s train acc: 59.228 test loss:1.31142 test acc: 59.01\n",
      "\n",
      "Epoch: 34\n",
      "Train loss:1.31096 time: 9 s train acc: 59.234 test loss:1.40948 test acc: 57.93\n",
      "\n",
      "Epoch: 35\n",
      "Train loss:1.32185 time: 9 s train acc: 59.478 test loss:1.34794 test acc: 58.98\n",
      "\n",
      "Epoch: 36\n",
      "Train loss:1.31137 time: 9 s train acc: 60.048 test loss:1.39393 test acc: 58.21\n",
      "\n",
      "Epoch: 37\n",
      "Train loss:1.32846 time: 9 s train acc: 59.652 test loss:1.33026 test acc: 59.56\n",
      "\n",
      "Epoch: 38\n",
      "Train loss:1.31875 time: 9 s train acc: 59.83 test loss:1.34867 test acc: 58.84\n",
      "\n",
      "Epoch: 39\n",
      "Train loss:1.31815 time: 9 s train acc: 60.178 test loss:1.34619 test acc: 58.9\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         60.18%\n",
      "Testing Accuracy          58.90%\n",
      "Train-Test Gap            1.28%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 40\n",
      "Train loss:1.31896 time: 9 s train acc: 59.93 test loss:1.31817 test acc: 59.53\n",
      "\n",
      "Epoch: 41\n",
      "Train loss:1.34461 time: 9 s train acc: 59.378 test loss:1.35511 test acc: 59.01\n",
      "\n",
      "Epoch: 42\n",
      "Train loss:1.29911 time: 9 s train acc: 60.266 test loss:1.32645 test acc: 59.63\n",
      "\n",
      "Epoch: 43\n",
      "Train loss:1.27991 time: 9 s train acc: 61.13 test loss:1.34225 test acc: 60.07\n",
      "\n",
      "Epoch: 44\n",
      "Train loss:1.28483 time: 9 s train acc: 61.078 test loss:1.31251 test acc: 60.32\n",
      "\n",
      "Epoch: 45\n",
      "Train loss:1.26601 time: 9 s train acc: 61.302 test loss:1.33456 test acc: 60.43\n",
      "\n",
      "Epoch: 46\n",
      "Train loss:1.28104 time: 9 s train acc: 61.188 test loss:1.31380 test acc: 60.35\n",
      "\n",
      "Epoch: 47\n",
      "Train loss:1.26723 time: 9 s train acc: 61.66 test loss:1.32578 test acc: 60.16\n",
      "\n",
      "Epoch: 48\n",
      "Train loss:1.28007 time: 9 s train acc: 61.676 test loss:1.33956 test acc: 60.16\n",
      "\n",
      "Epoch: 49\n",
      "Train loss:1.25967 time: 9 s train acc: 61.94 test loss:1.30788 test acc: 60.29\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         61.94%\n",
      "Testing Accuracy          60.29%\n",
      "Train-Test Gap            1.65%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 50\n",
      "Train loss:1.22223 time: 9 s train acc: 62.964 test loss:1.27583 test acc: 61.41\n",
      "\n",
      "Epoch: 51\n",
      "Train loss:1.21286 time: 9 s train acc: 63.354 test loss:1.27335 test acc: 61.7\n",
      "\n",
      "Epoch: 52\n",
      "Train loss:1.21045 time: 9 s train acc: 63.546 test loss:1.27520 test acc: 61.85\n",
      "\n",
      "Epoch: 53\n",
      "Train loss:1.21178 time: 9 s train acc: 63.544 test loss:1.27111 test acc: 61.97\n",
      "\n",
      "Epoch: 54\n",
      "Train loss:1.20793 time: 9 s train acc: 63.646 test loss:1.28594 test acc: 62.08\n",
      "\n",
      "Epoch: 55\n",
      "Train loss:1.21189 time: 9 s train acc: 63.648 test loss:1.27701 test acc: 62.42\n",
      "\n",
      "Epoch: 56\n",
      "Train loss:1.20812 time: 9 s train acc: 63.812 test loss:1.27613 test acc: 62.41\n",
      "\n",
      "Epoch: 57\n",
      "Train loss:1.20882 time: 9 s train acc: 63.998 test loss:1.27291 test acc: 62.32\n",
      "\n",
      "Epoch: 58\n",
      "Train loss:1.20922 time: 9 s train acc: 64.018 test loss:1.27444 test acc: 62.24\n",
      "\n",
      "Epoch: 59\n",
      "Train loss:1.20925 time: 9 s train acc: 64.118 test loss:1.27730 test acc: 62.44\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         64.12%\n",
      "Testing Accuracy          62.44%\n",
      "Train-Test Gap            1.68%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 60\n",
      "Train loss:1.20409 time: 9 s train acc: 64.098 test loss:1.28269 test acc: 62.34\n",
      "\n",
      "Epoch: 61\n",
      "Train loss:1.20602 time: 9 s train acc: 64.17 test loss:1.27227 test acc: 62.51\n",
      "\n",
      "Epoch: 62\n",
      "Train loss:1.20184 time: 9 s train acc: 64.182 test loss:1.27367 test acc: 62.73\n",
      "\n",
      "Epoch: 63\n",
      "Train loss:1.20147 time: 9 s train acc: 64.38 test loss:1.27726 test acc: 62.6\n",
      "\n",
      "Epoch: 64\n",
      "Train loss:1.20275 time: 9 s train acc: 64.36 test loss:1.27687 test acc: 62.77\n",
      "\n",
      "Epoch: 65\n",
      "Train loss:1.20403 time: 9 s train acc: 64.49 test loss:1.27258 test acc: 62.65\n",
      "\n",
      "Epoch: 66\n",
      "Train loss:1.20066 time: 9 s train acc: 64.468 test loss:1.27204 test acc: 63.19\n",
      "\n",
      "Epoch: 67\n",
      "Train loss:1.19826 time: 9 s train acc: 64.458 test loss:1.27274 test acc: 62.92\n",
      "\n",
      "Epoch: 68\n",
      "Train loss:1.19891 time: 9 s train acc: 64.61 test loss:1.27574 test acc: 62.87\n",
      "\n",
      "Epoch: 69\n",
      "Train loss:1.20018 time: 9 s train acc: 64.448 test loss:1.27039 test acc: 62.9\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         64.45%\n",
      "Testing Accuracy          62.90%\n",
      "Train-Test Gap            1.55%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 70\n",
      "Train loss:1.19553 time: 9 s train acc: 64.66 test loss:1.26438 test acc: 62.99\n",
      "\n",
      "Epoch: 71\n",
      "Train loss:1.19585 time: 9 s train acc: 64.556 test loss:1.26888 test acc: 63.17\n",
      "\n",
      "Epoch: 72\n",
      "Train loss:1.19931 time: 9 s train acc: 64.624 test loss:1.27778 test acc: 63.43\n",
      "\n",
      "Epoch: 73\n",
      "Train loss:1.19782 time: 9 s train acc: 64.688 test loss:1.27229 test acc: 63.55\n",
      "\n",
      "Epoch: 74\n",
      "Train loss:1.19440 time: 9 s train acc: 64.822 test loss:1.26651 test acc: 63.38\n",
      "\n",
      "Epoch: 75\n",
      "Train loss:1.19043 time: 9 s train acc: 64.892 test loss:1.26570 test acc: 63.39\n",
      "\n",
      "Epoch: 76\n",
      "Train loss:1.18894 time: 9 s train acc: 64.952 test loss:1.26586 test acc: 63.41\n",
      "\n",
      "Epoch: 77\n",
      "Train loss:1.18912 time: 9 s train acc: 64.952 test loss:1.26525 test acc: 63.5\n",
      "\n",
      "Epoch: 78\n",
      "Train loss:1.18831 time: 9 s train acc: 64.92 test loss:1.26429 test acc: 63.42\n",
      "\n",
      "Epoch: 79\n",
      "Train loss:1.18757 time: 9 s train acc: 64.934 test loss:1.26502 test acc: 63.39\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         64.93%\n",
      "Testing Accuracy          63.39%\n",
      "Train-Test Gap            1.54%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 80\n",
      "Train loss:1.18806 time: 9 s train acc: 64.986 test loss:1.26371 test acc: 63.38\n",
      "\n",
      "Epoch: 81\n",
      "Train loss:1.18718 time: 9 s train acc: 64.9 test loss:1.26405 test acc: 63.52\n",
      "\n",
      "Epoch: 82\n",
      "Train loss:1.18782 time: 9 s train acc: 64.912 test loss:1.26554 test acc: 63.45\n",
      "\n",
      "Epoch: 83\n",
      "Train loss:1.18772 time: 9 s train acc: 64.954 test loss:1.26543 test acc: 63.49\n",
      "\n",
      "Epoch: 84\n",
      "Train loss:1.18794 time: 9 s train acc: 64.958 test loss:1.26634 test acc: 63.55\n",
      "\n",
      "Epoch: 85\n",
      "Train loss:1.18737 time: 9 s train acc: 64.982 test loss:1.26601 test acc: 63.5\n",
      "\n",
      "Epoch: 86\n",
      "Train loss:1.18818 time: 9 s train acc: 64.888 test loss:1.26521 test acc: 63.48\n",
      "\n",
      "Epoch: 87\n",
      "Train loss:1.18831 time: 9 s train acc: 64.984 test loss:1.26583 test acc: 63.48\n",
      "\n",
      "Epoch: 88\n",
      "Train loss:1.18742 time: 9 s train acc: 64.948 test loss:1.26468 test acc: 63.43\n",
      "\n",
      "Epoch: 89\n",
      "Train loss:1.18774 time: 9 s train acc: 64.958 test loss:1.26554 test acc: 63.42\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         64.96%\n",
      "Testing Accuracy          63.42%\n",
      "Train-Test Gap            1.54%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n",
      "\n",
      "Epoch: 90\n",
      "Train loss:1.18792 time: 9 s train acc: 64.996 test loss:1.26472 test acc: 63.47\n",
      "\n",
      "Epoch: 91\n",
      "Train loss:1.18706 time: 9 s train acc: 64.938 test loss:1.26609 test acc: 63.53\n",
      "\n",
      "Epoch: 92\n",
      "Train loss:1.18658 time: 9 s train acc: 64.986 test loss:1.26465 test acc: 63.4\n",
      "\n",
      "Epoch: 93\n",
      "Train loss:1.18656 time: 9 s train acc: 65.01 test loss:1.26480 test acc: 63.48\n",
      "\n",
      "Epoch: 94\n",
      "Train loss:1.18702 time: 9 s train acc: 65.032 test loss:1.26304 test acc: 63.43\n",
      "\n",
      "Epoch: 95\n",
      "Train loss:1.18643 time: 9 s train acc: 65.026 test loss:1.26444 test acc: 63.45\n",
      "\n",
      "Epoch: 96\n",
      "Train loss:1.18641 time: 9 s train acc: 65.024 test loss:1.26333 test acc: 63.4\n",
      "\n",
      "Epoch: 97\n",
      "Train loss:1.18629 time: 9 s train acc: 65.024 test loss:1.26694 test acc: 63.53\n",
      "\n",
      "Epoch: 98\n",
      "Train loss:1.18675 time: 9 s train acc: 65.068 test loss:1.26533 test acc: 63.43\n",
      "\n",
      "Epoch: 99\n",
      "Train loss:1.18660 time: 9 s train acc: 65.09 test loss:1.26574 test acc: 63.5\n",
      "\n",
      "==== CIFAR-10 Results Summary ====\n",
      "Configuration             Accuracy       \n",
      "----------------------------------------\n",
      "Training Accuracy         65.09%\n",
      "Testing Accuracy          63.50%\n",
      "Train-Test Gap            1.59%\n",
      "\n",
      "Learning curve plots saved to dp_cifar10_accuracy_curves.png and dp_cifar10_final_accuracy.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate models with and without differential privacy on CIFAR-10.\n",
    "    \n",
    "    This function sets the training parameters, computes the noise scale to meet the privacy budget,\n",
    "    trains two models (one with DP and one without), records performance metrics, and plots the learning curves.\n",
    "    \"\"\"\n",
    "    # Set training parameters.\n",
    "    use_cuda = True\n",
    "    batchsize = 1000\n",
    "    n_epoch = 100\n",
    "    eps = 8.0\n",
    "    delta = 1e-5\n",
    "    clip = 5.0\n",
    "    lr = 0.1\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0\n",
    "    \n",
    "    # Lists to store metrics for plotting.\n",
    "    train_acc_dp = []\n",
    "    test_acc_dp = []\n",
    "    train_acc_no_dp = []\n",
    "    test_acc_no_dp = []\n",
    "    epochs = []\n",
    "    \n",
    "    # Load CIFAR-10 data.\n",
    "    trainloader, testloader, n_training, n_test = get_data_loader('cifar10', batchsize=batchsize)\n",
    "    print('# of training examples:', n_training, '# of testing examples:', n_test)\n",
    "    \n",
    "    # Compute the noise scale based on the target privacy budget.\n",
    "    print('\\n==> Computing noise scale for privacy budget (%.1f, %f)-DP' % (eps, delta))\n",
    "    sampling_prob = batchsize / n_training\n",
    "    steps = int(n_epoch / sampling_prob)\n",
    "    sigma, eps = get_sigma(sampling_prob, steps, eps, delta, rgp=False)\n",
    "    noise_multiplier = sigma\n",
    "    print('noise scale:', noise_multiplier, 'privacy guarantee:', eps)\n",
    "    \n",
    "    # Train the model with differential privacy.\n",
    "    print('\\n==> Creating and training model with DP')\n",
    "    global best_acc\n",
    "    best_acc = 0\n",
    "    \n",
    "    net_dp = resnet20()\n",
    "    net_dp.cuda()\n",
    "    net_dp = extend(net_dp)\n",
    "    \n",
    "    # Display total number of model parameters.\n",
    "    num_params = sum(p.numel() for p in net_dp.parameters())\n",
    "    print('total number of parameters:', num_params / (10**6), 'M')\n",
    "    \n",
    "    # Use CrossEntropyLoss with sum reduction for proper scaling.\n",
    "    loss_func_dp = nn.CrossEntropyLoss(reduction='sum')\n",
    "    loss_func_dp = extend(loss_func_dp)\n",
    "    \n",
    "    # Set the optimizer for the DP model.\n",
    "    optimizer_dp = optim.SGD(net_dp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    print('\\n==> Start training with DP')\n",
    "    for epoch in range(n_epoch):\n",
    "        epochs.append(epoch)\n",
    "        current_lr = adjust_learning_rate(optimizer_dp, lr, epoch, all_epoch=n_epoch)\n",
    "        train_loss, train_acc = train(epoch, net_dp, trainloader, optimizer_dp, loss_func_dp,\n",
    "                                      n_training, batchsize, clip, noise_multiplier, private=True)\n",
    "        test_loss, test_acc = test(epoch, net_dp, testloader, loss_func_dp, private=True)\n",
    "        \n",
    "        # Store metrics for plotting.\n",
    "        train_acc_dp.append(train_acc)\n",
    "        test_acc_dp.append(test_acc)\n",
    "    \n",
    "    # Reset best accuracy for the non-DP model.\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Train the model without differential privacy.\n",
    "    print('\\n==> Creating and training model without DP')\n",
    "    net_no_dp = resnet20()\n",
    "    net_no_dp.cuda()\n",
    "    \n",
    "    # Use standard mean reduction in loss for non-DP training.\n",
    "    loss_func_no_dp = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    optimizer_no_dp = optim.SGD(net_no_dp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    print('\\n==> Start training without DP')\n",
    "    for epoch in range(n_epoch):\n",
    "        current_lr = adjust_learning_rate(optimizer_no_dp, lr, epoch, all_epoch=n_epoch)\n",
    "        train_loss, train_acc = train(epoch, net_no_dp, trainloader, optimizer_no_dp, loss_func_no_dp,\n",
    "                                      n_training, batchsize, clip, noise_multiplier, private=False)\n",
    "        test_loss, test_acc = test(epoch, net_no_dp, testloader, loss_func_no_dp, private=False)\n",
    "        \n",
    "        # Store metrics.\n",
    "        train_acc_no_dp.append(train_acc)\n",
    "        test_acc_no_dp.append(test_acc)\n",
    "    \n",
    "    # Plot the learning curves and final results.\n",
    "    plot_learning_curves(epochs, train_acc_dp, test_acc_dp, train_acc_no_dp, test_acc_no_dp)\n",
    "\n",
    "def plot_learning_curves(epochs, train_acc_dp, test_acc_dp, train_acc_no_dp, test_acc_no_dp):\n",
    "    \"\"\"\n",
    "    Plot training and testing accuracy curves as well as the accuracy gap between training and testing,\n",
    "    comparing models trained with DP versus without DP.\n",
    "    \n",
    "    Generates and saves two figures:\n",
    "      1. A line plot comparing accuracies over epochs.\n",
    "      2. A bar chart summarizing the final accuracies.\n",
    "    \"\"\"\n",
    "    # Figure 1: Accuracy curves.\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Subplot: Training and testing accuracies.\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_acc_dp, 'o-', color='blue', label='Training with DP')\n",
    "    plt.plot(epochs, train_acc_no_dp, 'o-', color='green', label='Training without DP')\n",
    "    plt.plot(epochs, test_acc_dp, 'o-', color='orange', label='Testing with DP')\n",
    "    plt.plot(epochs, test_acc_no_dp, 'o-', color='red', label='Testing without DP')\n",
    "    plt.title('CIFAR-10: Accuracy vs. Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Subplot: Train-Test accuracy gap.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    gap_dp = [train - test for train, test in zip(train_acc_dp, test_acc_dp)]\n",
    "    gap_no_dp = [train - test for train, test in zip(train_acc_no_dp, test_acc_no_dp)]\n",
    "    plt.plot(epochs, gap_dp, 'o-', color='purple', label='Train-Test Gap with DP')\n",
    "    plt.plot(epochs, gap_no_dp, 'o-', color='brown', label='Train-Test Gap without DP')\n",
    "    plt.title('CIFAR-10: Train-Test Accuracy Gap')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gap (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cifar10_accuracy_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Figure 2: Final accuracy bar chart.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    final_accuracies = [\n",
    "        train_acc_dp[-1], \n",
    "        test_acc_dp[-1], \n",
    "        train_acc_no_dp[-1], \n",
    "        test_acc_no_dp[-1]\n",
    "    ]\n",
    "    configurations = [\n",
    "        'Training with DP', \n",
    "        'Testing with DP', \n",
    "        'Training without DP', \n",
    "        'Testing without DP'\n",
    "    ]\n",
    "    colors = ['blue', 'orange', 'green', 'red']\n",
    "    plt.bar(configurations, final_accuracies, color=colors)\n",
    "    plt.title('CIFAR-10: Final Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=15)\n",
    "    for i, v in enumerate(final_accuracies):\n",
    "        plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cifar10_final_accuracy_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n==== CIFAR-10 Results Summary ====\")\n",
    "    print(f\"{'Configuration':<25} {'Accuracy':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Training with DP':<25} {train_acc_dp[-1]:.2f}%\")\n",
    "    print(f\"{'Testing with DP':<25} {test_acc_dp[-1]:.2f}%\")\n",
    "    print(f\"{'Training without DP':<25} {train_acc_no_dp[-1]:.2f}%\")\n",
    "    print(f\"{'Testing without DP':<25} {test_acc_no_dp[-1]:.2f}%\")\n",
    "    print(f\"{'DP Train-Test Gap':<25} {train_acc_dp[-1] - test_acc_dp[-1]:.2f}%\")\n",
    "    print(f\"{'Non-DP Train-Test Gap':<25} {train_acc_no_dp[-1] - test_acc_no_dp[-1]:.2f}%\")\n",
    "    print(\"\\nPlots saved to cifar10_accuracy_comparison.png and cifar10_final_accuracy_comparison.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataPrivacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
