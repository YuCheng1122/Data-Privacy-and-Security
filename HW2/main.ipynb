{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9267f611",
   "metadata": {},
   "source": [
    "Create Domain.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df2c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from ucimlrepo) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/tommy/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "adult = fetch_ucirepo(id=2)\n",
    "df = adult.data.features.assign(income=adult.data.targets.squeeze())\n",
    "df = df.replace(\"?\", np.nan).fillna(\"0\")\n",
    "\n",
    "domain = {}\n",
    "\n",
    "skip = {\"fnlwgt\", \"capital-gain\", \"capital-loss\", \"age\", \"hours-per-week\"}\n",
    "for col in df.columns:\n",
    "    if col in skip:\n",
    "        continue\n",
    "    domain[col] = sorted(df[col].astype(str).unique().tolist())\n",
    "\n",
    "domain[\"age_bin\"] = sorted(pd.cut(\n",
    "    df[\"age\"].astype(int),\n",
    "    bins=[17,25,35,50,65, df[\"age\"].astype(int).max()+1],\n",
    "    labels=False, include_lowest=True\n",
    ").unique().tolist())\n",
    "\n",
    "domain[\"hours_bin\"] = sorted(pd.cut(\n",
    "    df[\"hours-per-week\"].astype(int),\n",
    "    bins=[0,20,40,60,80, df[\"hours-per-week\"].astype(int).max()+1],\n",
    "    labels=False, include_lowest=True\n",
    ").unique().tolist())\n",
    "\n",
    "domain[\"income\"] = [\"<=50K\", \">50K\"]\n",
    "\n",
    "with open(\"domain.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(domain, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "for k, v in domain.items():\n",
    "    print(f\"{k} ({len(v)}): {v[:5]} …\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26dc6a",
   "metadata": {},
   "source": [
    "Create discrete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09656962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretized dataset saved to adult_discretized.csv\n",
      "Final shape: (48842, 12)\n",
      "\n",
      "Sample of discretized data:\n",
      "   workclass  education  education-num  marital-status  occupation  \\\n",
      "0          7          9              4               4           1   \n",
      "1          6          9              4               2           4   \n",
      "2          4         11             15               0           6   \n",
      "3          4          1             13               2           6   \n",
      "4          4          9              4               2          10   \n",
      "\n",
      "   relationship  race  sex  native-country  income  age_bin  hours_bin  \n",
      "0             1     4    1              39       0        2          1  \n",
      "1             0     4    1              39       0        2          0  \n",
      "2             1     4    1              39       0        2          1  \n",
      "3             0     2    1              39       0        3          1  \n",
      "4             5     2    0               5       0        1          1  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from scipy.constants import ounce\n",
    "\n",
    "def discretize_adults(input_csv, domain_json, output_csv):\n",
    "    # Load domain information\n",
    "    with open(domain_json, encoding=\"utf-8\") as f:\n",
    "        domain = json.load(f)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Replace missing values with \"0\"\n",
    "    df = df.replace(\"?\", pd.NA).fillna(\"0\")\n",
    "    \n",
    "    # Create age bins\n",
    "    if \"age\" in df.columns and \"age_bin\" in domain:\n",
    "        df[\"age_bin\"] = pd.cut(\n",
    "            df[\"age\"].astype(int),\n",
    "            bins=[17,25,35,50,65, df[\"age\"].astype(int).max()+1],\n",
    "            labels=False, include_lowest=True\n",
    "        )\n",
    "    \n",
    "    # Create hours bins\n",
    "    if \"hours-per-week\" in df.columns and \"hours_bin\" in domain:\n",
    "        df[\"hours_bin\"] = pd.cut(\n",
    "            df[\"hours-per-week\"].astype(int),\n",
    "            bins=[0,20,40,60,80, df[\"hours-per-week\"].astype(int).max()+1],\n",
    "            labels=False, include_lowest=True\n",
    "        )\n",
    "    \n",
    "    # Copy dataframe for coding\n",
    "    df_code = df.copy()\n",
    "    \n",
    "    # Map each column according to domain\n",
    "    for col, vals in domain.items():\n",
    "        if col in df_code.columns:\n",
    "            mapping = {str(v): i for i, v in enumerate(vals)}\n",
    "            df_code[col] = df_code[col].astype(str).map(mapping).fillna(0).astype(int)\n",
    "    \n",
    "    # Keep only columns in domain.json\n",
    "    df_code = df_code[list(domain.keys())]\n",
    "    \n",
    "    # Remove specific columns if they exist\n",
    "    columns_to_drop = [\"fnlwgt\", \"capital-gain\", \"capital-loss\", \"age\", \"hours-per-week\"]\n",
    "    df_code = df_code.drop(columns=[c for c in columns_to_drop if c in df_code.columns])\n",
    "    \n",
    "    # Save the discretized dataset\n",
    "    df_code.to_csv(output_csv, index=False)\n",
    "    print(f\"Discretized dataset saved to {output_csv}\")\n",
    "    print(f\"Final shape: {df_code.shape}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample of discretized data:\")\n",
    "    print(df_code.head())\n",
    "\n",
    "input_csv = \"adult.csv\"  \n",
    "domain_json = \"domain.json\"\n",
    "output_csv = \"adult_discretized.csv\"\n",
    "discretize_adults(input_csv, domain_json, output_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f982a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mechanism import Mechanism\n",
    "import mbi\n",
    "from mbi import Domain, Factor, Dataset\n",
    "import matrix\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy import sparse, optimize\n",
    "from privacy.research.hyperparameters_2022.rdp_accountant import compute_rdp, get_privacy_spent\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "def transform_data(data, supports):\n",
    "    df = data.df.copy()\n",
    "    newdom = {}\n",
    "    for col in data.domain:\n",
    "        support = supports[col]\n",
    "        size = support.sum()\n",
    "        newdom[col] = int(size)\n",
    "        if size < support.size:\n",
    "            newdom[col] += 1\n",
    "        mapping = {}\n",
    "        idx = 0\n",
    "        for i in range(support.size):\n",
    "            mapping[i] = size\n",
    "            if support[i]:\n",
    "                mapping[i] = idx\n",
    "                idx += 1\n",
    "        assert idx == size\n",
    "        df[col] = df[col].map(mapping)\n",
    "    newdom = Domain.fromdict(newdom)\n",
    "    return Dataset(df, newdom)\n",
    "\n",
    "def reverse_data(data, supports):\n",
    "    df = data.df.copy()\n",
    "    newdom = {}\n",
    "    for col in data.domain:\n",
    "        support = supports[col]\n",
    "        mx = support.sum()\n",
    "        newdom[col] = int(support.size)\n",
    "        idx, extra = np.where(support)[0], np.where(~support)[0]\n",
    "        mask = df[col] == mx\n",
    "        if extra.size == 0:\n",
    "            pass\n",
    "        else:\n",
    "            df.loc[mask, col] = np.random.choice(extra, mask.sum())\n",
    "        df.loc[~mask, col] = idx[df.loc[~mask, col]]\n",
    "    newdom = Domain.fromdict(newdom)\n",
    "    return Dataset(df, newdom)\n",
    "\n",
    "def moments_calibration(round1, round2, eps, delta):\n",
    "    # round1: L2 sensitivity of round1 queries\n",
    "    # round2: L2 sensitivity of round2 queries\n",
    "    # works as long as eps >= 0.01; if larger, increase orders\n",
    "    orders = range(2, 4096)\n",
    "\n",
    "    def obj(sigma):\n",
    "        rdp1 = compute_rdp(1.0, sigma/round1, 1, orders)\n",
    "        rdp2 = compute_rdp(1.0, sigma/round2, 1, orders)\n",
    "        rdp = rdp1 + rdp2\n",
    "        privacy = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "        return privacy[0] - eps + 1e-8\n",
    "    low = 1.0\n",
    "    high = 1.0\n",
    "    while obj(low) < 0:\n",
    "        low /= 2.0\n",
    "    while obj(high) > 0:\n",
    "        high *= 2.0\n",
    "    sigma = optimize.bisect(obj, low, high)\n",
    "    assert obj(sigma) - 1e-8 <= 0, 'not differentially private' # true eps <= requested eps\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1d338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook inconsistent for education-num\n",
      "Setup completed!\n",
      "Round1 queries: 12 one-way marginals\n",
      "Round2 queries: 19 multi-way marginals\n",
      "\n",
      "Sample two-way relationships:\n",
      "  1. ('workclass', 'income')\n",
      "  2. ('education', 'income')\n",
      "  3. ('marital-status', 'income')\n",
      "  4. ('occupation', 'income')\n",
      "  5. ('age_bin', 'income')\n",
      "開始測量過程...\n",
      "噪聲水平 (sigma): 0.7596780918920558\n",
      "workclass: 域大小=9, 支持集大小=0\n",
      "education: 域大小=16, 支持集大小=0\n",
      "education-num: 域大小=17, 支持集大小=16\n",
      "marital-status: 域大小=7, 支持集大小=0\n",
      "occupation: 域大小=15, 支持集大小=0\n",
      "relationship: 域大小=6, 支持集大小=0\n",
      "race: 域大小=5, 支持集大小=0\n",
      "sex: 域大小=2, 支持集大小=0\n",
      "native-country: 域大小=42, 支持集大小=0\n",
      "income: 域大小=2, 支持集大小=0\n",
      "age_bin: 域大小=5, 支持集大小=5\n",
      "hours_bin: 域大小=5, 支持集大小=5\n",
      "測量完成！生成了 31 個測量結果\n",
      "錯誤: unhashable type: 'numpy.ndarray'\n",
      "嘗試備用方案...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    205\u001b[39m adults_setup.save    = \u001b[33m'\u001b[39m\u001b[33madult_synthetic.csv\u001b[39m\u001b[33m'\u001b[39m            \n\u001b[32m    207\u001b[39m adults_setup.measure()\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m synth_data = \u001b[43madults_setup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m synth_data.to_csv(adults_setup.save, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mAdultsSetup.postprocess\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m theta = engine.model.mle(marginals)\n\u001b[32m    167\u001b[39m engine.potentials = theta\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m engine.marginals = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbelief_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Create checkpoint file path\u001b[39;00m\n\u001b[32m    171\u001b[39m checkpt = \u001b[38;5;28mself\u001b[39m.save[:-\u001b[32m4\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m-checkpt.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Homework/Data-Privacy-and-Security/HW2/graphical_model.py:165\u001b[39m, in \u001b[36mGraphicalModel.belief_propagation\u001b[39m\u001b[34m(self, potentials, logZ)\u001b[39m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m         tau = beliefs[i]\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     messages[(i,j)] = \u001b[43mtau\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     beliefs[j] += messages[(i,j)]\n\u001b[32m    168\u001b[39m cl = \u001b[38;5;28mself\u001b[39m.cliques[\u001b[32m0\u001b[39m]      \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/mbi/factor.py:82\u001b[39m, in \u001b[36mFactor.logsumexp\u001b[39m\u001b[34m(self, attrs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrs: Sequence[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mFactor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscipy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspecial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogsumexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/mbi/factor.py:71\u001b[39m, in \u001b[36mFactor._aggregate\u001b[39m\u001b[34m(self, fn, attrs)\u001b[39m\n\u001b[32m     69\u001b[39m attrs = \u001b[38;5;28mself\u001b[39m.domain.attrs \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m attrs\n\u001b[32m     70\u001b[39m axes = \u001b[38;5;28mself\u001b[39m.domain.axes(attrs)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m values = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m newdom = \u001b[38;5;28mself\u001b[39m.domain.marginalize(attrs)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Factor(newdom, values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/ops/special.py:79\u001b[39m, in \u001b[36mlogsumexp\u001b[39m\u001b[34m(a, axis, b, keepdims, return_sign, where)\u001b[39m\n\u001b[32m     77\u001b[39m   a_arr, = promote_args_inexact(\u001b[33m\"\u001b[39m\u001b[33mlogsumexp\u001b[39m\u001b[33m\"\u001b[39m, a)\n\u001b[32m     78\u001b[39m   b_arr = a_arr  \u001b[38;5;66;03m# for type checking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m pos_dims, dims = \u001b[43m_reduction_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m amax = jnp.max(a_arr.real, axis=dims, keepdims=keepdims, where=where, initial=-jnp.inf)\n\u001b[32m     81\u001b[39m amax = lax.stop_gradient(lax.select(jnp.isfinite(amax), amax, lax.full_like(amax, \u001b[32m0\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:176\u001b[39m, in \u001b[36m_reduction_dims\u001b[39m\u001b[34m(a, axis)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, (np.ndarray, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    175\u001b[39m   axis = (axis,)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m canon_axis = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_canonicalize_axis_allow_named\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                   \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(canon_axis) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(canon_axis)):\n\u001b[32m    179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mduplicate value in \u001b[39m\u001b[33m'\u001b[39m\u001b[33maxis\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:176\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, (np.ndarray, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    175\u001b[39m   axis = (axis,)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m canon_axis = \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_canonicalize_axis_allow_named\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m                    \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m axis)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(canon_axis) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(canon_axis)):\n\u001b[32m    179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mduplicate value in \u001b[39m\u001b[33m'\u001b[39m\u001b[33maxis\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:169\u001b[39m, in \u001b[36m_canonicalize_axis_allow_named\u001b[39m\u001b[34m(x, rank)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_canonicalize_axis_allow_named\u001b[39m(x, rank):\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmaybe_named_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_canonicalize_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/util.py:509\u001b[39m, in \u001b[36mmaybe_named_axis\u001b[39m\u001b[34m(axis, if_pos, if_named)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    508\u001b[39m   named = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m if_named(axis) \u001b[38;5;28;01mif\u001b[39;00m named \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mif_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/numpy/reductions.py:169\u001b[39m, in \u001b[36m_canonicalize_axis_allow_named.<locals>.<lambda>\u001b[39m\u001b[34m(i)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_canonicalize_axis_allow_named\u001b[39m(x, rank):\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m maybe_named_axis(x, \u001b[38;5;28;01mlambda\u001b[39;00m i: \u001b[43m_canonicalize_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m name: name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DataPrivacy/lib/python3.12/site-packages/jax/_src/util.py:376\u001b[39m, in \u001b[36mcanonicalize_axis\u001b[39m\u001b[34m(axis, num_dims)\u001b[39m\n\u001b[32m    374\u001b[39m axis = operator.index(axis)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m -num_dims <= axis < num_dims:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maxis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for array of dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis < \u001b[32m0\u001b[39m:\n\u001b[32m    378\u001b[39m   axis = axis + num_dims\n",
      "\u001b[31mValueError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mbi import Domain, Dataset\n",
    "from match3 import Match3\n",
    "import mbi.estimation as estimation \n",
    "import matrix\n",
    "from scipy import sparse\n",
    "from privacy.research.hyperparameters_2022 import rdp_accountant\n",
    "import random\n",
    "from inference import FactoredInference\n",
    "\n",
    "class AdultsSetup(Match3):\n",
    "    def __init__(self, dataset_path, specs_path, iters=1000):\n",
    "        super().__init__(dataset_path, specs_path, iters=iters)\n",
    "        \n",
    "        with open('domain.json', 'r') as f:\n",
    "            domain_dict = json.load(f)\n",
    "            \n",
    "        self.epsilon = 1.0 \n",
    "        self.delta = 2.2820544e-12\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"為 Adult 數據集定制的 setup 方法\"\"\"\n",
    "        self.round1 = list(self.domain.attrs)\n",
    "        \n",
    "        self.round2 = [\n",
    "            ('workclass', 'income'),\n",
    "            ('education', 'income'),\n",
    "            ('marital-status', 'income'),\n",
    "            ('occupation', 'income'),\n",
    "            ('age_bin', 'income'),\n",
    "            ('hours_bin', 'income'),\n",
    "            ('sex', 'income'),\n",
    "            ('education', 'occupation'),\n",
    "            ('age_bin', 'occupation'),\n",
    "            ('sex', 'occupation'),\n",
    "            ('marital-status', 'relationship'),\n",
    "            ('native-country', 'income'),\n",
    "            ('race', 'income'),\n",
    "            \n",
    "            ('age_bin', 'education', 'income'),\n",
    "            ('age_bin', 'occupation', 'income'),\n",
    "            ('sex', 'occupation', 'income'),\n",
    "            ('education', 'occupation', 'income'),\n",
    "            ('workclass', 'occupation', 'income'),\n",
    "            ('marital-status', 'relationship', 'income')\n",
    "        ]\n",
    "        \n",
    "        domain_attrs = set(self.domain.attrs)\n",
    "        self.round2 = [rel for rel in self.round2 if all(attr in domain_attrs for attr in rel)]\n",
    "        \n",
    "        print(\"Setup completed!\")\n",
    "        print(f\"Round1 queries: {len(self.round1)} one-way marginals\")\n",
    "        print(f\"Round2 queries: {len(self.round2)} multi-way marginals\")\n",
    "        \n",
    "        print(\"\\nSample two-way relationships:\")\n",
    "        for i, rel in enumerate(self.round2[:5]):\n",
    "            print(f\"  {i+1}. {rel}\")\n",
    "    \n",
    "    def measure(self):\n",
    "        \"\"\"為 Adult 數據集定制的測量方法\"\"\"\n",
    "        print(\"開始測量過程...\")\n",
    "        data = self.load_data()\n",
    "        \n",
    "        # 計算噪聲參數 (基於隱私預算)\n",
    "        sigma = moments_calibration(1.0, 1.0, self.epsilon, self.delta)\n",
    "        print('噪聲水平 (sigma):', sigma)\n",
    "\n",
    "        # 計算邊際的權重，使 L2 敏感度為 1\n",
    "        weights = np.ones(len(self.round1))\n",
    "        weights /= np.linalg.norm(weights)  # 現在 L2 範數 = 1\n",
    "\n",
    "        supports = {}\n",
    "  \n",
    "        self.measurements = []\n",
    "        # 處理 Round1 查詢（一階邊際）\n",
    "        for col, wgt in zip(self.round1, weights):\n",
    "            # 噪聲添加步驟\n",
    "            proj = (col,)\n",
    "            hist = data.project(proj).datavector()\n",
    "            noise = sigma * np.random.randn(hist.size)\n",
    "            y = wgt * hist + noise\n",
    "          \n",
    "            # 後處理步驟\n",
    "            # 確定支持集（頻率高於噪聲閾值的值）\n",
    "            sup = y >= 3 * sigma\n",
    "            supports[col] = sup\n",
    "            print(f\"{col}: 域大小={hist.size}, 支持集大小={sup.sum()}\")\n",
    "\n",
    "            # 重構查詢結果\n",
    "            if sup.sum() == y.size:\n",
    "                y2 = y\n",
    "                I2 = matrix.Identity(y.size)\n",
    "            else:\n",
    "                # 合併低頻值\n",
    "                y2 = np.append(y[sup], y[~sup].sum())\n",
    "                I2 = np.ones(y2.size)\n",
    "                I2[-1] = 1.0 / np.sqrt(y.size - y2.size + 1.0)\n",
    "                y2[-1] /= np.sqrt(y.size - y2.size + 1.0)\n",
    "                I2 = sparse.diags(I2)\n",
    "\n",
    "            # 保存測量結果\n",
    "            self.measurements.append((I2, y2/wgt, 1.0/wgt, proj))\n",
    "\n",
    "        # 保存支持集信息\n",
    "        self.supports = supports \n",
    "        \n",
    "        # 基於支持集轉換數據以減少稀疏性 \n",
    "        data = transform_data(data, supports)\n",
    "        self.domain = data.domain\n",
    "\n",
    "        # 過濾掉太大的查詢 (避免計算爆炸)\n",
    "        self.round2 = [cl for cl in self.round2 if self.domain.size(cl) < 1e6]\n",
    "        weights = np.ones(len(self.round2))\n",
    "        weights /= np.linalg.norm(weights)  # 現在 L2 範數 = 1\n",
    "   \n",
    "        # 處理 Round2 查詢（二階和三階邊際）\n",
    "        for proj, wgt in zip(self.round2, weights):\n",
    "            # 噪聲添加步驟\n",
    "            hist = data.project(proj).datavector()\n",
    "            Q = matrix.Identity(hist.size)\n",
    "            noise = sigma * np.random.randn(Q.shape[0])\n",
    "            y = wgt * Q.dot(hist) + noise\n",
    "            self.measurements.append((Q, y/wgt, 1.0/wgt, proj))\n",
    "            \n",
    "        print(f\"測量完成！生成了 {len(self.measurements)} 個測量結果\")\n",
    "     \n",
    "    def postprocess(self):\n",
    "        \"\"\"Implements McKenna's approach for generating synthetic data\"\"\"\n",
    "        import mbi.callbacks\n",
    "        from functools import reduce\n",
    "        from callbacks import Logger\n",
    "        from mbi import Factor\n",
    "        import os\n",
    "        \n",
    "        # Initialize the inference engine\n",
    "        domain = self.domain\n",
    "        engine = FactoredInference(domain,\n",
    "                                structural_zeros={},\n",
    "                                iters=500,\n",
    "                                log=True,\n",
    "                                warm_start=True,\n",
    "                                elim_order=None)  # You can set a custom elimination order if needed\n",
    "        \n",
    "        self.engine = engine\n",
    "        cb = Logger(engine)\n",
    "        \n",
    "        # Warmup for faster convergence (initialize with one-way marginals)\n",
    "        engine._setup(self.measurements, None)\n",
    "        oneway = {}\n",
    "        for i in range(len(self.round1)):\n",
    "            p = self.round1[i]\n",
    "            y = self.measurements[i][1]\n",
    "            y = np.maximum(y, 1)  # Ensure no negative values\n",
    "            y /= y.sum()  # Normalize\n",
    "            oneway[p] = Factor(self.domain.project(p), y)\n",
    "        \n",
    "        # Initialize model with product of one-way marginals\n",
    "        marginals = {}\n",
    "        for cl in engine.model.cliques:\n",
    "            marginals[cl] = reduce(lambda x,y: x*y, [oneway[p] for p in cl])\n",
    "        \n",
    "        # Maximum likelihood estimation for the initial potentials\n",
    "        theta = engine.model.mle(marginals)\n",
    "        engine.potentials = theta\n",
    "        engine.marginals = engine.model.belief_propagation(theta)\n",
    "        \n",
    "        # Create checkpoint file path\n",
    "        checkpt = self.save[:-4] + '-checkpt.csv'\n",
    "        \n",
    "        # Run inference in batches\n",
    "        for i in range(self.iters // 500):\n",
    "            engine.infer(self.measurements, engine='MD', callback=cb)\n",
    "            \n",
    "            # Save checkpoint every 4 batches (2000 iterations)\n",
    "            if i % 4 == 3:\n",
    "                self.synthetic = engine.model.synthetic_data()\n",
    "                self.synthetic = reverse_data(self.synthetic, self.supports)\n",
    "                self.transform_domain()\n",
    "                self.synthetic.to_csv(checkpt, index=False)\n",
    "        \n",
    "        # Clean up checkpoint file if it exists\n",
    "        if os.path.exists(checkpt):\n",
    "            os.remove(checkpt)\n",
    "        \n",
    "        # Generate final synthetic data\n",
    "        self.synthetic = engine.model.synthetic_data()\n",
    "        self.synthetic = reverse_data(self.synthetic, self.supports)\n",
    "        \n",
    "        # Transform back to original domain\n",
    "        self.transform_domain()\n",
    "        \n",
    "        return self.synthetic\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = 'adult_discretized.csv'\n",
    "    specs_path   = 'adult_specs.json'\n",
    "    adults_setup = AdultsSetup(dataset_path, specs_path)\n",
    "    adults_setup.setup()\n",
    "\n",
    "    adults_setup.epsilon = 10\n",
    "    adults_setup.delta   = 1e-5\n",
    "    adults_setup.save    = 'adult_synthetic.csv'            \n",
    "\n",
    "    adults_setup.measure()\n",
    "    synth_data = adults_setup.postprocess()\n",
    "    synth_data.to_csv(adults_setup.save, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f353f2",
   "metadata": {},
   "source": [
    "Test synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda97899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass         0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "native-country    0\n",
      "income            0\n",
      "age_bin           0\n",
      "hours_bin         0\n",
      "dtype: int64\n",
      "(48842, 11) (48842,)\n"
     ]
    }
   ],
   "source": [
    "#Load the synthetic data\n",
    "synth_data = pd.read_csv(\"adult_synthetic.csv\")\n",
    "\n",
    "#Check if have null\n",
    "print(synth_data.isnull().sum())\n",
    "\n",
    "df_x = synth_data.drop(columns=[\"income\"])\n",
    "df_y = synth_data[\"income\"]\n",
    "print(df_x.shape, df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94215870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
      "       'relationship', 'race', 'sex', 'native-country'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from operator import le\n",
    "from sklearn import preprocessing\n",
    "\n",
    "seed = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=seed)\n",
    "\n",
    "cate = x_train.select_dtypes(include=\"object\").columns\n",
    "print(cate)\n",
    "\n",
    "\n",
    "for col in cate:\n",
    "    all_values = pd.concat([x_train[col], x_test[col]]).unique()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(all_values)\n",
    "\n",
    "    x_train[col] = le.transform(x_train[col])\n",
    "    x_test[col] = le.transform(x_test[col])\n",
    "\n",
    "\n",
    "y_train_encoded = (y_train == '>50K').astype(int)\n",
    "y_test_encoded = (y_test == '>50K').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df440c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       workclass  education  education-num  marital-status  occupation  \\\n",
      "37193          4          3              7               6          14   \n",
      "31093          4          5              1               6          12   \n",
      "33814          5          6              7               4          11   \n",
      "14500          2          7              7               2           1   \n",
      "23399          3          6              1               2           7   \n",
      "\n",
      "       relationship  race  sex  native-country  age_bin  hours_bin  \n",
      "37193             5     4    0              23        2          4  \n",
      "31093             5     3    0              17        0          4  \n",
      "33814             5     1    1              31        2          4  \n",
      "14500             4     0    1              13        2          4  \n",
      "23399             4     0    0              14        0          4  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(39073, 11) (9769, 11) (39073,) (9769,)\n",
      "(39073, 11) (9769, 11)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[-1.26824480e-03 -9.76806708e-01  9.98491144e-01  1.49844831e+00\n",
      "   1.63283854e+00  1.46538973e+00  1.41529484e+00 -9.93647490e-01\n",
      "   2.07768525e-01  9.98491144e-01  0.00000000e+00]\n",
      " [-1.26824480e-03 -5.42622241e-01 -1.00151114e+00  1.49844831e+00\n",
      "   1.16944252e+00  1.46538973e+00  7.07058326e-01 -9.93647490e-01\n",
      "  -2.86551756e-01 -1.00151114e+00  0.00000000e+00]\n",
      " [ 3.85873389e-01 -3.25530007e-01  9.98491144e-01  4.99013827e-01\n",
      "   9.37744514e-01  1.46538973e+00 -7.09414703e-01  1.00639312e+00\n",
      "   8.66862234e-01  9.98491144e-01  0.00000000e+00]\n",
      " [-7.75551512e-01 -1.08437773e-01  9.98491144e-01 -5.00420653e-01\n",
      "  -1.37923557e+00  8.80888328e-01 -1.41765122e+00  1.00639312e+00\n",
      "  -6.16098610e-01  9.98491144e-01  0.00000000e+00]\n",
      " [-3.88409878e-01 -3.25530007e-01 -1.00151114e+00 -5.00420653e-01\n",
      "   1.09524793e-02  8.80888328e-01 -1.41765122e+00 -9.93647490e-01\n",
      "  -5.33711897e-01 -1.00151114e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.head())\n",
    "print(type(x_train))\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "scalar = preprocessing.StandardScaler()\n",
    "x_train = scalar.fit_transform(x_train)\n",
    "x_test = scalar.transform(x_test)\n",
    "\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(type(x_train), type(x_test))\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfbac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Rate: 0.02508125815780715\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.50      0.45      0.48      4918\n",
      "        >50K       0.49      0.54      0.52      4851\n",
      "\n",
      "    accuracy                           0.50      9769\n",
      "   macro avg       0.50      0.50      0.50      9769\n",
      "weighted avg       0.50      0.50      0.50      9769\n",
      "\n",
      "Accuracy: 0.49728733749616133\n",
      "Precision: 0.49758596707742253\n",
      "Recall: 0.49728733749616133\n",
      "F1 Score: 0.4963648007217378\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "with open('RandomForeast.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "    random_forest = RandomForestClassifier(\n",
    "        random_state=seed,\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        )\n",
    "    random_forest.fit(x_train, y_train)\n",
    "    y_train_pred = random_forest.predict(x_train)\n",
    "    train_error_rate = 1 - accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"Train Error Rate: {train_error_rate}\")\n",
    "    y_pred = random_forest.predict(x_test)\n",
    "\n",
    "print(f\"Report: {classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataPrivacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
